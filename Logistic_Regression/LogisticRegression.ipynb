{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"},"colab":{"name":"LogisticRegression.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"Ampom1Xa14g7","colab_type":"code","colab":{}},"source":["# importing libraries\n","import nltk\n","import pandas as pd\n","import numpy as np \n","import re\n","from gensim.models import Word2Vec\n","from io import StringIO\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n","from sklearn.feature_selection import chi2\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","import seaborn as sns\n","from sklearn.svm import LinearSVC\n","from sklearn import metrics\n","\n","# importing the stop words\n","stop_words = set(nltk.corpus.stopwords.words('english')) \n","\n","# importing the sample dataset from Emotion_Phrases.csv\n","data = pd.read_csv(\"datasets/Emotion_Phrases.csv\", header=None)\n","\n","# adding the coloumns names\n","data.columns = ['emotions', 'text']\n","\n","# getting the informations of data\n","data.info()\n","\n","# converting the emotions to the ids\n","data['emo_id'] = data['emotions'].factorize()[0]\n","\n","# print(data.head(10))\n","# print(data['emo_id'])\n","\n","# dropped the duplicates and sorted it\n","emo_id_df = data[['emotions', 'emo_id']].drop_duplicates().sort_values('emo_id')\n","\n","# converted it into a dictionary \n","emo_to_id = dict(emo_id_df.values)\n","\n","# representing the dataset with a plot\n","fig = plt.figure()\n","data.groupby('emotions').text.count().plot.bar(ylim=0)\n","plt.show()\n","\n","# converted the data into the dictionary\n","data_dict = data.to_dict()\n","\n","sw_rem = []\n","txt = []\n","\n","# preprocessed the database:\n","#       *   Removed the words other than A-Z and a-z and space\n","#       *   Tokenize the words\n","#       *   Convert the tokenized words into a list\n","for k in range(len(data_dict['text'])):\n","    data_dict['text'][k] = re.sub('([^A-Za-z ])', '', data_dict['text'][k])\n","    txt.append(nltk.tokenize.word_tokenize(data_dict['text'][k]))\n","    for j in range(len(txt[k])):\n","        if txt[k][j] not in stop_words: \n","           sw_rem.append(txt[k][j])\n","    data_dict['text'][k] = \" \".join(sw_rem)\n","    sw_rem = [] \n","data = pd.DataFrame.from_dict(data_dict)\n","\n","# using tfidf vectorizer to convert the words into vectors as words that occur more frequently in one document and less frequently in other documents should be given more importance as they are more useful for classification.\n","tfidf = TfidfVectorizer(sublinear_tf=True, min_df=1, norm='l2', encoding='utf-8', ngram_range=(1,2), stop_words='english')\n","l_data = data.text.tolist()\n","fitted_vectorizer=tfidf.fit(l_data)\n","features=fitted_vectorizer.transform(l_data)\n","labels = data.emo_id\n","\n","# split train and test data\n","X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, data.index, test_size=0.33, random_state=0)\n","\n","# instantiate the model \n","model = LinearSVC()\n","\n","# train the model\n","model.fit(X_train, y_train)\n","\n","# used the train model to predict the dataset\n","y_pred = model.predict(X_test)\n","\n","# confusion matrix\n","conf_mat = metrics.confusion_matrix(y_test, y_pred)\n","fig, ax = plt.subplots(figsize=(8,6))\n","sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=emo_id_df.emotions.values, yticklabels=emo_id_df.emotions.values)\n","plt.ylabel('Actual')\n","plt.xlabel('Predicted')\n","plt.show()\n","\n","# generated an accuracy report based on the tests\n","print(metrics.classification_report(y_test, y_pred, target_names=data['emotions'].unique()))\n","\n","# print(clf.predict(count_vect.transform([\"Angry Angry Angry Angry\"])))"],"execution_count":0,"outputs":[]}]}