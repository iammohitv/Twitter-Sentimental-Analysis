{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"classification.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"OI4BQwfUAVfZ","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from nltk.corpus import stopwords\n","from textblob import Word\n","import re\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import accuracy_score\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uRaD15OhBLXg","colab_type":"code","outputId":"f4018f7c-f930-45a5-d68f-6698da77d3a6","colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C_ul1MnRAlGl","colab_type":"code","outputId":"7edce253-7fe4-4592-ea97-bab6b1488eee","colab":{"base_uri":"https://localhost:8080/","height":252}},"source":["data = pd.read_csv('/content/drive/My Drive/ML_Final_Submission/Dataset/text_emotion.csv')\n","\n","data2 = pd.read_csv('/content/drive/My Drive/ML_Final_Submission/Dataset/Emotion_Phrases.csv')\n","\n","data2.columns = ['sentiment', 'content']\n","\n","#words = pd.read_csv('Emotion_Words.csv')\n","\n","#words.columns = ['emo', 'class']\n","\n","data = data.drop('author', axis=1)\n","\n","data = data.drop('tweet_id', axis=1)\n","\n","data = data.append(data2, ignore_index=True)\n","\n","# for x in range(len(data['sentiment'])):\n","#     for y in range(len(words['emo'])):\n","#         if data['sentiment'][x] == words['emo'][y]:\n","#             data['sentiment'][x] = words['class'][y]\n","\n","#\n","\n","\n","\n","# Dropping rows with other emotion labels\n","#data = data.drop(data[data.sentiment == 'anger'].index)\n","data = data.drop(data[data.sentiment == 'boredom'].index)\n","data = data.drop(data[data.sentiment == 'enthusiasm'].index)\n","data = data.drop(data[data.sentiment == 'empty'].index)\n","data = data.drop(data[data.sentiment == 'fun'].index)\n","data = data.drop(data[data.sentiment == 'relief'].index)\n","data = data.drop(data[data.sentiment == 'surprise'].index)\n","data = data.drop(data[data.sentiment == 'love'].index)\n","data = data.drop(data[data.sentiment == 'hate'].index)\n","data = data.drop(data[data.sentiment == 'neutral'].index)\n","data = data.drop(data[data.sentiment == 'worry'].index)\n","\n","print(data)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["      sentiment                                            content\n","1       sadness  Layin n bed with a headache  ughhhh...waitin o...\n","2       sadness                Funeral ceremony...gloomy friday...\n","6       sadness  I should be sleep, but im not! thinking about ...\n","8       sadness            @charviray Charlene my love. I miss you\n","9       sadness         @kelcouch I'm sorry  at least it's Friday?\n","...         ...                                                ...\n","47646     anger  Two years back someone invited me to be the tu...\n","47647   sadness  I had taken the responsibility to do something...\n","47648   disgust  I was at home and I heard a loud sound of spit...\n","47649     shame  I did not do the homework that the teacher had...\n","47650     guilt  I had shouted at my younger brother and he was...\n","\n","[18135 rows x 2 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3A1P6l-WAonH","colab_type":"code","colab":{}},"source":["# Making all letters lowercase\n","import nltk\n","nltk.download('stopwords')\n","\n","data['content'] = data['content'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n","\n","# Removing Punctuation, Symbols\n","data['content'] = data['content'].str.replace('[^\\w\\s]',' ')\n","\n","# Removing Stop Words using NLTK\n","stop = stopwords.words('english')\n","data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n","\n","#Lemmatisation\n","data['content'] = data['content'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n","#Correcting Letter Repetitions\n","\n","def de_repeat(text):\n","    pattern = re.compile(r\"(.)\\1{2,}\")\n","    return pattern.sub(r\"\\1\\1\", text)\n","\n","data['content'] = data['content'].apply(lambda x: \" \".join(de_repeat(x) for x in x.split()))\n","\n","# Code to find the top 10,000 rarest words appearing in the data\n","freq = pd.Series(' '.join(data['content']).split()).value_counts()[-10000:]\n","\n","# Removing all those rarely appearing words from the data\n","freq = list(freq.index)\n","data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n","\n","#Encoding output labels 'sadness' as '1' & 'happiness' as '0'\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"INEyj3V4B9Na","colab_type":"code","outputId":"6261279b-4714-4d73-80b3-5d30affb0392","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["lbl_enc = preprocessing.LabelEncoder()\n","y = lbl_enc.fit_transform(data.sentiment.values)\n","print(lbl_enc.classes_)\n","# Splitting into training and testing data in 90:10 ratio\n","X_train, X_val, y_train, y_val = train_test_split(data.content.values, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)\n","\n","# Extracting TF-IDF parameters\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['anger' 'disgust' 'fear' 'guilt' 'happiness' 'joy' 'sadness' 'shame']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_xchzwn_CG4h","colab_type":"code","colab":{}},"source":["tfidf = TfidfVectorizer(max_features=1000, analyzer='word',ngram_range=(1,3))\n","X_train_tfidf = tfidf.fit_transform(X_train)\n","X_val_tfidf = tfidf.fit_transform(X_val)\n","\n","# Extracting Count Vectors Parameters\n","count_vect = CountVectorizer(analyzer='word')\n","count_vect.fit(data['content'])\n","X_train_count =  count_vect.transform(X_train)\n","X_val_count =  count_vect.transform(X_val)\n","\n","# Model 1: Multinomial Naive Bayes Classifier\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"awcjKJTMCL8L","colab_type":"code","outputId":"3cdae7a9-27a6-4b32-baf0-daf17b73374a","colab":{"base_uri":"https://localhost:8080/","height":218}},"source":["nb = MultinomialNB()\n","nb.fit(X_train_tfidf, y_train)\n","y_pred = nb.predict(X_val_tfidf)\n","print('naive bayes tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n","# naive bayes tfidf accuracy 0.5289017341040463\n","\n","# Model 2: Linear SVM\n","lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n","lsvm.fit(X_train_tfidf, y_train)\n","y_pred = lsvm.predict(X_val_tfidf)\n","print('svm using tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n","# svm tfidf accuracy 0.5404624277456648\n","\n","# Model 3: logistic regression\n","logreg = LogisticRegression(C=1)\n","logreg.fit(X_train_tfidf, y_train)\n","y_pred = logreg.predict(X_val_tfidf)\n","print('log reg tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n","# log reg tfidf accuracy 0.5443159922928709\n","\n","# Model 4: Random Forest Classifier\n","rf = RandomForestClassifier(n_estimators=500)\n","rf.fit(X_train_tfidf, y_train)\n","y_pred = rf.predict(X_val_tfidf)\n","print('random forest tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n","# random forest tfidf accuracy 0.5385356454720617\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["naive bayes tfidf accuracy 0.30705622932745313\n","svm using tfidf accuracy 0.3054024255788313\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["log reg tfidf accuracy 0.3054024255788313\n","random forest tfidf accuracy 0.3296582138919515\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MtDyuKE_CRaf","colab_type":"code","outputId":"9d4b8ac1-cda6-4933-c65b-68d91e66d8dd","colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["## Building models using count vectors feature\n","# Model 1: Multinomial Naive Bayes Classifier\n","nb = MultinomialNB()\n","nb.fit(X_train_count, y_train)\n","y_pred = nb.predict(X_val_count)\n","print('naive bayes count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n","# naive bayes count vectors accuracy 0.7764932562620424\n","\n","# Model 2: Linear SVM\n","lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n","lsvm.fit(X_train_count, y_train)\n","y_pred = lsvm.predict(X_val_count)\n","print('lsvm using count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n","# lsvm using count vectors accuracy 0.7928709055876686\n","\n","# Model 3: Logistic Regression\n","logreg = LogisticRegression(C=1)\n","logreg.fit(X_train_count, y_train)\n","y_pred = logreg.predict(X_val_count)\n","print('log reg count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n","# log reg count vectors accuracy 0.7851637764932563\n","\n","# Model 4: Random Forest Classifier\n","rf = RandomForestClassifier(n_estimators=500)\n","rf.fit(X_train_count, y_train)\n","y_pred = rf.predict(X_val_count)\n","print('random forest with count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n","# random forest with count vectors accuracy 0.7524084778420038\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["naive bayes count vectors accuracy 0.6466372657111357\n","lsvm using count vectors accuracy 0.6830209481808158\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["log reg count vectors accuracy 0.6819184123484013\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1oQxeWukCSFv","colab_type":"code","colab":{}},"source":["\n","#Below are 8 random statements. The first 4 depict happiness. The last 4 depict sadness\n","\n","tweets = pd.DataFrame(['I am very happy today! The atmosphere looks cheerful',\n","'Things are looking great. It was such a good day',\n","'Success is right around the corner. Lets celebrate this victory',\n","'Everything is more beautiful when you experience them with a smile!',\n","'Now this is my worst, okay? But I am gonna get better.',\n","'I am tired, boss. Tired of being on the road, lonely as a sparrow in the rain. I am tired of all the pain I feel',\n","'This is quite depressing. I am filled with sorrow',\n","'His death broke my heart. It was a sad day'])\n","\n","# Doing some preprocessing on these tweets as done before\n","tweets[0] = tweets[0].str.replace('[^\\w\\s]',' ')\n","from nltk.corpus import stopwords\n","stop = stopwords.words('english')\n","tweets[0] = tweets[0].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n","from textblob import Word\n","tweets[0] = tweets[0].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n","\n","# Extracting Count Vectors feature from our tweets\n","tweet_count = count_vect.transform(tweets[0])\n","\n","#Predicting the emotion of the tweet using our already trained linear SVM\n","tweet_pred = lsvm.predict(tweet_count)\n","print(tweet_pred)"],"execution_count":0,"outputs":[]}]}