# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ni0ml9Ng19KH66rWCCbF90EpGAl4nd6i

# Importing Required libraries
"""

#Required Libraries

import nltk
import pandas as pd
import numpy as np 
import re
from gensim.models import Word2Vec
from io import StringIO
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer
from sklearn.feature_selection import chi2
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
import seaborn as sns
from sklearn.svm import LinearSVC
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.model_selection import cross_val_score

"""# Importing Required Packages"""

import nltk
nltk.download('wordnet')

import nltk
nltk.download('punkt')

import nltk
nltk.download('averaged_perceptron_tagger')

import nltk
nltk.download('stopwords')

from google.colab import drive
drive.mount('/content/drive')

"""# Data Loading and Preprocessing   
1.Removed the words other than A-Z and a-z and space

---


2.Tokenize the words

---


3.Convert the tokenized words into a list
"""

# importing the stop words
stop_words = set(nltk.corpus.stopwords.words('english')) 

lemmatizer = nltk.stem.WordNetLemmatizer()

dt = []

# importing the sample dataset from Emotion_Phrases.csv
an = open('/content/drive/My Drive/Dataset/ANGER_Phrases.txt', 'r')
anger = an.readlines()
an.close()
for i in anger:
    dt.append([i, 'anger'])

fe = open('/content/drive/My Drive/Dataset/FEAR_Phrases.txt', 'r')
fear = fe.readlines()
fe.close()
for i in fear:
    dt.append([i, 'fear'])

jo = open('/content/drive/My Drive/Dataset/JOY_Phrases.txt', 'r')
joy = jo.readlines()
jo.close()
for i in joy:
    dt.append([i, 'joy'])

lo = open('/content/drive/My Drive/Dataset/LOVE_Phrases.txt', 'r')
love = lo.readlines()
lo.close()
for i in love:
    dt.append([i, 'love'])

sa = open('/content/drive/My Drive/Dataset/SADNESS_Phrases.txt', 'r')
sad = sa.readlines()
sa.close()
for i in sad:
    dt.append([i, 'sad'])

su = open('/content/drive/My Drive/Dataset/SURPRISE_Phrases.txt', 'r')
surprise = su.readlines()
su.close()
for i in surprise:
    dt.append([i, 'surprise'])

print(len(dt))

data = pd.DataFrame(dt)

# adding the coloumns names
data.columns = ['text', 'emotions']

# getting the informations of data
data.info()


# converting the emotions to the ids
data['emo_id'] = data['emotions'].factorize()[0]

# print(data.head(10))
# print(data['emo_id'])
#data.iloc[np.random.permutation(len(data))]
#data.reindex(np.random.permutation(data.index))

data = data.sample(frac=1)

#print(data)



# dropped the duplicates and sorted it
emo_id_df = data[['emotions', 'emo_id']].drop_duplicates().sort_values('emo_id')

# converted it into a dictionary 
emo_to_id = dict(emo_id_df.values)

# representing the dataset with a plot


# converted the data into the dictionary
data_dict = data.to_dict()

sw_rem = []
txt = []
tagged = []

# preprocessed the database:
#       *   Removed the words other than A-Z and a-z and space
#       *   Tokenize the words
#       *   Convert the tokenized words into a list
tagged = []
print(list(data_dict['text'].values())[:5])
for k in range(len(list(data_dict['text'])[:12000])):
    data_dict['text'][k] = re.sub('([^A-Za-z ])', '', data_dict['text'][k])
    txt.append(nltk.tokenize.word_tokenize(data_dict['text'][k]))
    tagged.append(nltk.pos_tag(txt[k]))
    for j in range(len(txt[k])):
        if (txt[k][j] not in stop_words) and (tagged[k][j][1] not in ['CC', 'CD', 'IN', 'LS', 'TO']): 
           sw_rem.append(lemmatizer.lemmatize(txt[k][j]))
    data_dict['text'][k] = " ".join(sw_rem)
    sw_rem = [] 
print(list(data_dict['text'].values())[:5])
data = pd.DataFrame.from_dict(data_dict)
#print(data)

"""## Representation of dataset with respective Emotions"""

fig = plt.figure()
data.groupby('emotions').text.count().plot.bar(ylim=0)
plt.show()

"""# Extracting features


1.   CountVectorizer:   We segment each text file into words. count no. of times each word occurs in each document(tweet) and finally assign each word an integer id. Each unique word in our dictionary will correspond to a feature
2.   TF:   CountVectorizer will give more weightage to longer documents than shorter documents. To avoid this, we can use frequency (TF - Term Frequencies) i.e. #count(word) / #Total words, in each document.
3. TF-IDF:    Finally, we can even reduce the weightage of more common words like (the, is, an etc.) which occurs in all document. This is called as TF-IDF i.e Term Frequency times inverse document frequency.
"""

count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(data.text[:12000])
#print(X_train_counts)
#print(X_train_counts.shape)


tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
from scipy import sparse
X_train_tfidf = sparse.lil_matrix(sparse.csr_matrix(X_train_tfidf)[:,0:300])
#print(X_train_tfidf)
print(X_train_tfidf.shape)

"""# Converting Sparse Matrix to Dense Matrix"""

labels = data.emo_id
#print(X_train_tfidf)
#print(features.get_shape())
features_d = X_train_tfidf.toarray()
#print(features_d)
rows = len(features_d)
columns = len(features_d[0])
print(rows)
print(columns)
############
X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X_train_tfidf[:12000], labels[:12000], data.index[:12000], test_size=0.33, random_state=0)

"""# PCA Analysis

## 1. Normalize Function
"""

def normalize(X):
 mu = np.mean(X, axis=0)
 std = np.std(X, axis=0)
 a = np.subtract(X,mu)
 new_std = np.where(std>0,std,1)
 Xbar = np.divide(a,new_std)
 # std_filled = std.copy()
 # std_filled[std==0] = 1.
 return Xbar, mu, std

"""## 2. Eig Function
  It will compute :  

1.   Eigen Values
2.   Eigen Vector
"""

def eig(S):
 eigvals, eigvecs = np.linalg.eig(S)
 idx = eigvals.argsort()[::-1]
 eigvals = eigvals[idx]
 eigvecs = eigvecs[:,idx]
 return (eigvals, eigvecs)

"""## 3. Projection_Matrix Function
Computes the projection matrix onto the space spanned by `B`
"""

def projection_matrix(B):
 P = np.matmul(B,B.T)
 #return np.eye(B.shape[0]) # <-- EDIT THIS to compute the projection matrix
 return P

"""# 4. PCA Function
*   Input
      1.   X: ndarray of size (N, D), where D is the dimension of the data, and N is the number of datapoints
      2.   num_components: the number of principal components to use.
*   Returns
      1.   X_reconstruct: ndarray of the reconstruction
  of X from the first `num_components` principal components.
      2.   sum_value: sum of eigen values from the first `num_components` principal components.
"""

def PCA(X, num_components):
 Xbar, mu, std = normalize(X)
 covariance = np.dot(Xbar.T,Xbar)
 S = covariance
 eigvals, eigvecs = eig(S)
 # eigen_trans = eigvecs.T
 # B = np.stack(eigen_trans[:,:num_components])
 sum_value = sum(eigvals[:num_components])
 B = np.stack(eigvecs[:,:num_components])
 P = np.matmul(B,B.T)
 X_reconstruct = np.matmul(P,X.T)
 X_reconstruct = X_reconstruct.T
 return X_reconstruct, sum_value

"""## 5. Normalizing Input Matrix
Normalization is necessary to make every variable in proportion with each other
"""

NUM_DATAPOINTS = 12000
data1 = (features_d [:NUM_DATAPOINTS]) / 255.
#print(data1)
Xbar, mu, std = normalize(data1)
print(Xbar.shape)
print(mu.shape)
print(std.shape)
#print(np.trace(Xbar))

"""## 6. Computing Covariance Matrix"""

covariance = np.matmul(Xbar.T,Xbar)
print(covariance.shape)
print(np.trace(covariance))
S = covariance
eigvals, eigvecs = eig(S)
print(eigvals[:20])
print(eigvecs)
print(eigvecs.shape[0])

num_components = 150

"""## 7. Computing Projection Matrix"""

B = np.stack(eigvecs[:,:num_components])   
print('Shape of matrix B: ', B.shape)
print('Trace of matrix B: ', np.trace(B))  

P = projection_matrix(B)  # finding projection matrix P
print('Shape of Projection matrix : ', P.shape)

X_reconstruct ,sum_value = PCA(Xbar, num_components)            # finding reconstructed X by selecting num_components principal components
print('Shape of X_reconstruct matrix : ', X_reconstruct.shape)  # printing shape of X_reconstruct matrix
print('Shape of normalized X matrix : ', Xbar.shape)            # printing shape of normalized X matrix

print()
print('X reconstruced : \n', X_reconstruct)

"""## 8. Our PCA function Vs sklearn library"""

from sklearn.preprocessing import StandardScaler
standardized_data = StandardScaler(with_mean=True,with_std=True)
standardized_data = standardized_data.fit_transform(Xbar)

print('Shape of Normalized input matrix computed using library : ' , standardized_data.shape)
print('Shape of Normalized input matrix computed using user functions : ' , Xbar.shape)


sample_data = standardized_data
covar_matrix = np.matmul(sample_data.T, sample_data)

print("Shape of covariance matrix computed using Library = ", covar_matrix.shape)
print("Shape of covariance matrix computed by user functions = ", S.shape)

print('The covariance matrix is : \n' ,covar_matrix)
print('The trace of covariance matrix : ', np.trace(covar_matrix))

"""## 9. Analysis of number of principal component"""

for num_component in range(1, 5):
 from sklearn.decomposition import PCA as SKPCA
# We can compute a standard solution given by scikit-learn's implementation of PCA
 pca = SKPCA(n_components=num_component, svd_solver='full')
 sklearn_reconst = pca.inverse_transform(pca.fit_transform(Xbar))
 
 reconst ,sum_value = PCA(Xbar, num_component)
 np.testing.assert_almost_equal(reconst, sklearn_reconst)
 print(np.square(reconst - sklearn_reconst).sum())

"""## 10. Computing loss and variance for every number of principal components"""

def mse(predict, actual):
 return np.square(predict - actual).sum(axis=1).mean()

loss = []
reconstructions = []
variance_values = []
print(sum_value)
# iterate over different numbers of principal components, and compute the MSE
for num_component in range(1, 140):
    reconst,sum_value = PCA(Xbar, num_component)
    error = mse(reconst, Xbar)
    reconst = reconst*std + mu
    reconstructions.append(reconst)
    # print('n = {:d}, reconstruction_error = {:f}'.format(num_component, error))
    loss.append((num_component, error))
    variance_values.append((num_component, sum_value))
reconstructions = np.asarray(reconstructions)
reconstructions = reconstructions * std + mu # "unnormalize" the reconstructed image
#print(reconstructions)
print(reconstructions.shape)
loss = np.asarray(loss)

import pandas as pd
# create a table showing the number of principal components and MSE
pd.DataFrame(loss).head(100)

"""## 11. Plotting MSE Vs Principal Components"""

fig, ax = plt.subplots()
ax.plot(loss[:,0], loss[:,1]);
ax.axhline(105, linestyle='--', color='r', linewidth=2);
ax.xaxis.set_ticks(np.arange(1, 130, 5));
ax.yaxis.set_ticks(np.arange(100, 300, 30));
ax.set(xlabel='num_components', ylabel='MSE', title='MSE vs number of principal components');

"""## 12. Plotting Explained Variance ratio Vs Principal Components"""

tot = sum(eigvals)
var_exp = [(i / tot)*100 for i in eigvals] # computing variance

plt.figure(figsize=(6, 4))#plotting results

plt.bar(range(len(var_exp)), var_exp, alpha=1 , align='center', label='individual explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.xlim(0,100)
plt.legend(loc='best')
plt.tight_layout()

# taking random value of M for analysis
print('Eigen values sorted in decreasing order:')
for i in eigvals:
    print(i)
print('Top M (where M=2) Eigenvalues : ', eigvals[:num_components])             # printing top M eigenvalues
print()
print('M eigenvectors correspondong to top M eigenvalues: \n', eigvecs[:,:num_components])  # printing top M eigenvectors corresponding to top M eigenvalues

print()
print('Shape of one eigenvector : ', eigvecs.shape[0])

"""## 13. Plotting Eigen Values vs index (decreasing order)"""

# Plotting sorted eigen values (in decreasing order) v/s its index 
eigvals = np.asarray(eigvals)      # converting to array
eig_df = pd.DataFrame(eigvals)     # making a dataframe with index and eigenvalues
eig_df.rename(columns={0 : 'eigenvalues'},  inplace = True)  #renaming the column of dataframe to eigenvalues
print(eig_df.head())        # printing all the eigenvalues ( 1 to D (total number of features i.e. 4))

fig, ax = plt.subplots()
ax.plot(eigvals);
ax.xaxis.set_ticks(np.arange(1, 5, 5));
ax.set(xlabel='index', ylabel='Eigen values', title='Eigen Values vs index (decreasing order)');
plt.xlim(0,220)

"""## 14. Plotting Variance on num_components Vs Number of Principal Components"""

variance_values = np.asarray(variance_values) # converting variance into variance
var_df = pd.DataFrame(variance_values) #convrting variance into a data frame 
var_df.rename(columns={0 : 'num_components', 1: 'Variance'},  inplace = True)  # renaming the column of dataframe to variance
print(var_df.head(150))

sns.set()
fig, ax = plt.subplots()
ax.plot(variance_values[:,0], variance_values[:,1]);
ax.xaxis.set_ticks(np.arange(1, 5, 5));
ax.set(xlabel='num_components', ylabel='Variance', title='Variance on Number of Principal Components');

for l in ax.lines:
    plt.setp(l,linewidth=4)

"""## 15. Analysis of MSE Error vs No. of principal components with non sorted eigenvalues"""

def PCA_unsorted(X, num_components):
  Xbar, mu, std = normalize(X) #normalizing our input matrix
  covariance = np.dot(Xbar.T,Xbar) # covariance
  S = covariance # s is covariance
  eigvals, eigvecs = np.linalg.eig(S) # calculating eigen values and eigen vectors
  for i in eigvals:
    print(i)
  sum_value = sum(eigvals[:num_components])#sum of all components, we have NOT TAKEN SORTED LIST
  B = np.stack(eigvecs[:,:num_components]) # calculating matrix B by taking summation of num_components
  P = np.matmul(B,B.T) #P = B * Btranspose
  X_reconstruct = np.matmul(P,X.T) #reconstructing matrix X
  X_reconstruct = X_reconstruct.T
  return X_reconstruct, sum_value

loss_unsorted = []
reconstructions_unsorted = []
variance_values_unsorted = []

data2 = (features_d [:NUM_DATAPOINTS]) / 255.
#print(data1)
Xbar1, mu1, std1 = normalize(data2)

# iterate over different numbers of principal components, and compute the MSE
for num_component in range(1, 130):
  reconst, sum_value = PCA_unsorted(Xbar1, num_component)  #reconst contains reconstructed Xbar, sum value contains summation of significant of num_component
  error = mse(reconst, Xbar1)
  reconst = reconst*std1 + mu1 #reconst is un-normalized data
  reconstructions_unsorted.append(reconst) # appending un-normalized data
  print('M = {:d}, reconstruction_error = {:f}'.format(num_component, error))
  loss_unsorted.append((num_component, error)) # loss for unsorted un normalized data
  variance_values_unsorted.append((num_component, sum_value)) # variance for unsorted un normalized data

"""## Plotting MSE vs No. of Principal components(Unsorted)"""

loss_unsorted = np.asarray(loss_unsorted) # loss unsorted as an array
loss_df = pd.DataFrame(loss_unsorted) # as an dataframe
loss_df.rename(columns={0 : 'num_components', 1: 'MSE'},  inplace = True)  #renaming the column of dataframe to MSE
print(loss_df)

fig, ax = plt.subplots()
ax.plot(loss_unsorted[:,0], loss_unsorted[:,1]);
ax.xaxis.set_ticks(np.arange(1, 5, 5));
ax.set(xlabel='num_components', ylabel='MSE', title='MSE vs no. of principal components(Unsorted)');

for l in ax.lines:
    plt.setp(l,linewidth=4)

